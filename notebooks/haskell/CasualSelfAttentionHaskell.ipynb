{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dfe4817d",
   "metadata": {
    "vscode": {
     "languageId": "haskell"
    }
   },
   "outputs": [],
   "source": [
    "{-# LANGUAGE RecordWildCards #-}\n",
    "import GHC.Generics\n",
    "import Torch\n",
    "import Torch.NN as NN\n",
    "import Torch.Functional as F\n",
    "import Torch.Functional.Internal as FI\n",
    "\n",
    "import Torch.TensorFactories\n",
    "import Torch.TensorOptions\n",
    "\n",
    "import Control.Monad (when)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96cf24b",
   "metadata": {},
   "source": [
    "## casualSelfAttentionInit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bf339a68",
   "metadata": {
    "vscode": {
     "languageId": "haskell"
    }
   },
   "outputs": [],
   "source": [
    "data Config = Config\n",
    "  { configNEmbd :: Int\n",
    "  , configNHead :: Int\n",
    "  , configBlockSize :: Int\n",
    "  } deriving (Show, Eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ee5146e2",
   "metadata": {
    "vscode": {
     "languageId": "haskell"
    }
   },
   "outputs": [],
   "source": [
    "data CasualSelfAttention = CasualSelfAttention\n",
    "  { cAttn :: Linear\n",
    "  , cProj :: Linear\n",
    "  , nHead :: Int\n",
    "  , nEmbd :: Int\n",
    "  , attentionBias :: Tensor\n",
    "  } deriving (Generic, Show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be6dd3b5",
   "metadata": {
    "vscode": {
     "languageId": "haskell"
    }
   },
   "outputs": [],
   "source": [
    "createCausalMask :: Int -> Tensor\n",
    "createCausalMask seqLen =\n",
    "  FI.reshape triangle [1, 1, seqLen, seqLen]\n",
    "  where\n",
    "    triangle = FI.tril onesMatrix 0\n",
    "    onesMatrix = ones' [seqLen, seqLen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3fb355c8",
   "metadata": {
    "vscode": {
     "languageId": "haskell"
    }
   },
   "outputs": [],
   "source": [
    "casualSelfAttentionInit :: Config -> IO CasualSelfAttention\n",
    "casualSelfAttentionInit Config{..} = do\n",
    "  \n",
    "  when (configNEmbd `mod` configNHead /= 0) $\n",
    "    error \"configNEmbd must be divisible by configNHead\"\n",
    "  \n",
    "  cAttn <- sample (LinearSpec configNEmbd (3 * configNEmbd))\n",
    "  cProj <- sample (LinearSpec configNEmbd configNEmbd)\n",
    "\n",
    "  \n",
    "  let attentionBias = createCausalMask configBlockSize\n",
    "                  \n",
    "  return CasualSelfAttention\n",
    "    { cAttn = cAttn\n",
    "    , cProj = cProj\n",
    "    , nHead = configNHead\n",
    "    , nEmbd = configNEmbd\n",
    "    , attentionBias = attentionBias\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a2875d",
   "metadata": {},
   "source": [
    "## scaledDotProductAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b67552f",
   "metadata": {
    "vscode": {
     "languageId": "haskell"
    }
   },
   "outputs": [],
   "source": [
    "scaledDotProductAttention :: Tensor -> Tensor -> Tensor -> Maybe Tensor -> Maybe Float -> Tensor\n",
    "scaledDotProductAttention q k v mask dropout = \n",
    "  let\n",
    "    scaleFactor = FI.sqrt (fromIntegral $ last $ shape k)\n",
    "    scores = FI.div (FI.matmul q (FI.transpose k (-2) (-1))) scaleFactor\n",
    "  \n",
    "    \n",
    "    maskedScores = case mask of\n",
    "      Just m -> scores * m + (1.0 - m) * (-1e10)\n",
    "      Nothing -> scores\n",
    "    \n",
    "    tyype = dtype q\n",
    "    weights = FI.softmax maskedScores (-1) tyype\n",
    "    \n",
    "    droppedWeights = case dropout of\n",
    "       Just rate -> weights\n",
    "       Nothing -> weights\n",
    "    \n",
    "    output = FI.matmul weights v\n",
    "  in\n",
    "    output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc3bba49",
   "metadata": {
    "vscode": {
     "languageId": "haskell"
    }
   },
   "outputs": [],
   "source": [
    "q <- randIO' [4,8,16,64]-- (B, nh, T, hs)\n",
    "k <- randIO' [4,8,16,64]-- (B, nh, T, hs)\n",
    "v <- randIO' [4,8,16,64]-- (B, nh, T, hs)\n",
    "mask <- randIO' [1, 1, 16, 16] -- (1, 1, T, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7316ca82",
   "metadata": {
    "vscode": {
     "languageId": "haskell"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,8,16,64]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = scaledDotProductAttention q k v (Just mask) Nothing\n",
    "shape output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d07be5",
   "metadata": {},
   "source": [
    "## casualSelfAttentionForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "894072a4",
   "metadata": {
    "vscode": {
     "languageId": "haskell"
    }
   },
   "outputs": [],
   "source": [
    "casualSelfAttentionForward :: CasualSelfAttention -> Tensor -> Tensor\n",
    "casualSelfAttentionForward CasualSelfAttention{..} x = let\n",
    "  shapes = shape x\n",
    "  batchSize = head shapes \n",
    "  seqLen = shapes !! 1\n",
    "  embedSize = shapes !! 2\n",
    "  headSize = Prelude.div embedSize nHead  \n",
    "  \n",
    "  projected = NN.linear cAttn x\n",
    "  \n",
    "\n",
    "  qkvList = FI.chunk projected 3 2 \n",
    "  q = head qkvList \n",
    "  k = qkvList !! 1\n",
    "  v = qkvList !! 2\n",
    "  \n",
    "  q' = FI.transpose (F.view [batchSize, seqLen, nHead, headSize] q ) 1 2\n",
    "  k' = FI.transpose (F.view [batchSize, seqLen, nHead, headSize] k) 1 2\n",
    "  v' = FI.transpose (F.view [batchSize, seqLen, nHead, headSize] v ) 1 2\n",
    "\n",
    "  \n",
    "  currentMask = createCausalMask seqLen\n",
    "  att = scaledDotProductAttention q' k' v' (Just currentMask) Nothing\n",
    "  \n",
    "  y1 = FI.transpose att 1 2\n",
    "  y2 = contiguous y1\n",
    "  y = F.view [batchSize, seqLen, embedSize] y2 \n",
    "  \n",
    "  output = NN.linear cProj y\n",
    "  in \n",
    "   output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d3187adb",
   "metadata": {
    "vscode": {
     "languageId": "haskell"
    }
   },
   "outputs": [],
   "source": [
    "nHead = 8\n",
    "nEmbed = 128\n",
    "blockSize = 32\n",
    "\n",
    "batchSize = 4\n",
    "seqLen = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf4dcf",
   "metadata": {
    "vscode": {
     "languageId": "haskell"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Input shape: [4,16,128]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "attentionModule <- casualSelfAttentionInit (Config nEmbed nHead blockSize)\n",
    "x_example <- randIO' [batchSize, seqLen, nEmbed] -- BATCH_SIZE, SEQ_LEN, N_EMBD\n",
    "putStrLn $ \"Input shape: \" ++ show (shape x_example) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "name": "",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
