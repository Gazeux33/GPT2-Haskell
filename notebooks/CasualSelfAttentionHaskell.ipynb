{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dfe4817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{-# LANGUAGE RecordWildCards #-}\n",
    "import GHC.Generics\n",
    "import Torch\n",
    "import Torch.NN as NN\n",
    "import Torch.Functional as F\n",
    "import Torch.Functional.Internal as FI\n",
    "\n",
    "import Torch.TensorFactories\n",
    "import Torch.TensorOptions\n",
    "\n",
    "import Control.Monad (when)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96cf24b",
   "metadata": {},
   "source": [
    "## casualSelfAttentionInit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bf339a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data Config = Config\n",
    "  { configNEmbd :: Int\n",
    "  , configNHead :: Int\n",
    "  , configBlockSize :: Int\n",
    "  } deriving (Show, Eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ee5146e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data CasualSelfAttention = CasualSelfAttention\n",
    "  { cAttn :: Linear\n",
    "  , cProj :: Linear\n",
    "  , nHead :: Int\n",
    "  , nEmbd :: Int\n",
    "  , attentionBias :: Tensor\n",
    "  } deriving (Generic, Show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be6dd3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "createCausalMask :: Int -> Tensor\n",
    "createCausalMask seqLen =\n",
    "  FI.reshape triangle [1, 1, seqLen, seqLen]\n",
    "  where\n",
    "    triangle = FI.tril onesMatrix 0\n",
    "    onesMatrix = ones' [seqLen, seqLen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3fb355c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "casualSelfAttentionInit :: Config -> IO CasualSelfAttention\n",
    "casualSelfAttentionInit Config{..} = do\n",
    "  \n",
    "  -- Vérification que nEmbd est divisible par nHead\n",
    "  when (configNEmbd `mod` configNHead /= 0) $\n",
    "    error \"configNEmbd doit être divisible par configNHead\"\n",
    "  \n",
    "  -- Initialisation des couches linéaires\n",
    "  cAttn <- sample (LinearSpec configNEmbd (3 * configNEmbd))\n",
    "  cProj <- sample (LinearSpec configNEmbd configNEmbd)\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "  -- Création du masque d'attention causal (triangulaire inférieur)\n",
    "  let attentionBias = createCausalMask configBlockSize\n",
    "  \n",
    "\n",
    "                      \n",
    "  return CasualSelfAttention\n",
    "    { cAttn = cAttn\n",
    "    , cProj = cProj\n",
    "    , nHead = configNHead\n",
    "    , nEmbd = configNEmbd\n",
    "    , attentionBias = attentionBias\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a2875d",
   "metadata": {},
   "source": [
    "## scaledDotProductAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b67552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Fonction utilitaire pour l'attention dot-product avec scaling\n",
    "scaledDotProductAttention :: Tensor -> Tensor -> Tensor -> Maybe Tensor -> Maybe Float -> Tensor\n",
    "scaledDotProductAttention q k v mask dropout = \n",
    "  let\n",
    "    -- Calcul des scores d'attention et mise à l'échelle\n",
    "    scaleFactor = FI.sqrt (fromIntegral $ last $ shape k)\n",
    "    scores = FI.div (FI.matmul q (FI.transpose k (-2) (-1))) scaleFactor\n",
    "  \n",
    "    \n",
    "    -- Application du masque causal si fourni\n",
    "    maskedScores = case mask of\n",
    "      Just m -> scores * m + (1.0 - m) * (-1e10)\n",
    "      Nothing -> scores\n",
    "    \n",
    "    -- Softmax sur les scores\n",
    "    tyype = dtype q\n",
    "    weights = FI.softmax maskedScores (-1) tyype\n",
    "    \n",
    "    -- Application du dropout si spécifié\n",
    "    droppedWeights = case dropout of\n",
    "       --Just rate -> dropout2d weights rate True\n",
    "       Just rate -> weights\n",
    "       Nothing -> weights\n",
    "    \n",
    "    -- Valeurs pondérées\n",
    "    output = FI.matmul weights v\n",
    "  in\n",
    "    output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc3bba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "q <- randIO' [4,8,16,64]-- (B, nh, T, hs)\n",
    "k <- randIO' [4,8,16,64]-- (B, nh, T, hs)\n",
    "v <- randIO' [4,8,16,64]-- (B, nh, T, hs)\n",
    "mask <- randIO' [1, 1, 16, 16] -- (1, 1, T, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7316ca82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,8,16,64]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = scaledDotProductAttention q k v (Just mask) Nothing\n",
    "shape output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d07be5",
   "metadata": {},
   "source": [
    "## casualSelfAttentionForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "894072a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "casualSelfAttentionForward :: CasualSelfAttention -> Tensor -> Tensor\n",
    "casualSelfAttentionForward CasualSelfAttention{..} x = let\n",
    "  -- Obtenir les dimensions du batch, sequence length et embedding size\n",
    "  shapes = shape x\n",
    "  batchSize = head shapes \n",
    "  seqLen = shapes !! 1\n",
    "  embedSize = shapes !! 2\n",
    "  headSize = Prelude.div embedSize nHead  -- Division d'entiers, pas de tenseurs\n",
    "  \n",
    "  -- Projeter l'entrée en query, key, value\n",
    "  -- Supposons que c'est une opération linéaire:\n",
    "  projected = NN.linear cAttn x\n",
    "  \n",
    "  -- Diviser en 3 parties selon la dimension 2\n",
    "  qkvList = FI.chunk projected 3 2 \n",
    "  q = head qkvList \n",
    "  k = qkvList !! 1\n",
    "  v = qkvList !! 2\n",
    "  \n",
    "  -- -- Reshape pour multi-têtes d'attention\n",
    "  q' = FI.transpose (F.view [batchSize, seqLen, nHead, headSize] q ) 1 2\n",
    "  k' = FI.transpose (F.view [batchSize, seqLen, nHead, headSize] k) 1 2\n",
    "  v' = FI.transpose (F.view [batchSize, seqLen, nHead, headSize] v ) 1 2\n",
    "\n",
    "  \n",
    "  currentMask = createCausalMask seqLen\n",
    "  -- -- Attention avec masque causal\n",
    "  att = scaledDotProductAttention q' k' v' (Just currentMask) Nothing\n",
    "  \n",
    "  -- -- Reshape pour la sortie\n",
    "  y1 = FI.transpose att 1 2\n",
    "  y2 = contiguous y1\n",
    "  y = F.view [batchSize, seqLen, embedSize] y2 \n",
    "  \n",
    "  -- -- Projection finale\n",
    "  output = NN.linear cProj y\n",
    "  in \n",
    "   output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d3187adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nHead = 8\n",
    "nEmbed = 128\n",
    "blockSize = 32\n",
    "\n",
    "batchSize = 4\n",
    "seqLen = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "14bf4dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Input shape: [4,16,128]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attentionModule <- casualSelfAttentionInit (Config nEmbed nHead blockSize)\n",
    "x_example <- randIO' [batchSize, seqLen, nEmbed] -- BATCH_SIZE, SEQ_LEN, N_EMBD\n",
    "putStrLn $ \"Input shape: \" ++ show (shape x_example) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "mimetype": "text/x-haskell",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "9.2.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
